{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Diffuser Stable Diffusion Jup\n",
        "A first implementation of semi complete tools about SD 1.5 and SDXL on jupyter-colab</br>\n",
        "##Guide:</br>\n",
        "<div>\n",
        "<ol>\n",
        "<li>Launch Installation requirements at beginning</li>\n",
        "<li>Launch Environment prepare after the previous step is ended and the instance has auto restarted</li>\n",
        "<li>Launch the Path selector</li>\n",
        "<li>Model selector\n",
        "<ol>\n",
        "•Download the model (it use Civitai)\n",
        "•Select the model\n",
        "</ol></li>\n",
        "<li>Possibile Adapters only SD 1.5</li>\n",
        "<li>Generate the images</li>\n",
        "<li>Detailer on generated images</li>\n",
        "</ol>\n",
        "</div>\n",
        "The implementation use a drive as storage.</br>\n",
        "\n",
        "##Features</br>\n",
        "\n",
        "<li>Text/IMG-2img  SD 1.5 and SDXL</li>\n",
        "<li>Multiple LORAS</li>\n",
        "<li>Single embedding model</li>\n",
        "<li>single controlnet SD 1.5</li>\n",
        "<li>basic detailer SD 1.5 and SDXL</li>\n",
        "</br>\n",
        "</br>\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/chri002/sd_1.5_diffuser/blob/main/sd_jup_diffuser.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" height=\"30px\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "aDNPOKt2DdlK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0cX26EZSZml"
      },
      "source": [
        "#Installation requirements\n",
        "This block restarts automatically, so don't start any other block before it finished and DON'T PUSH REBOOT!!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TSBxL3MMDgL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check code env\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "HOME = \"/content/drive/MyDrive/\"\n",
        "COMF = HOME + \"/sd_1.5_diffuser\"\n",
        "IP_ADAPTER = COMF + \"/IP-Adapter\"\n",
        "\n",
        "\n",
        "\n",
        "if not(os.path.exists(COMF)):\n",
        "   %cd {HOME}\n",
        "   !git clone https://github.com/chri002/sd_1.5_diffuser\n",
        "\n",
        "if not(os.path.exists(IP_ADAPTER)):\n",
        "   %cd {COMF}\n",
        "   !git clone https://github.com/tencent-ailab/IP-Adapter\n",
        "%cd \"/content/\"\n",
        "\n",
        "\n",
        "!pip install -r {COMF+\"/requirements.txt\"}"
      ],
      "metadata": {
        "id": "Um6apdY0bUZu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers -q"
      ],
      "metadata": {
        "id": "5vdAfKoUB24L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlBLd1vGIKkV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIAc0s5NIUo9"
      },
      "source": [
        "#Enviroment prepare\n",
        "Lunch and accept drive connection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vmDYKSo3aYJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOL2bSBkphpp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "HOME = \"/content/drive/MyDrive/\"\n",
        "COMF = HOME + \"/sd_1.5_diffuser\"\n",
        "IP_ADAPTER = COMF + \"/IP-Adapter\"\n",
        "\n",
        "sys.path.insert(0,COMF)\n",
        "sys.path.insert(1,IP_ADAPTER)\n",
        "\n",
        "import random\n",
        "import mediapy as media\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import autocast\n",
        "\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, AutoencoderTiny, StableDiffusionXLPipeline, DiffusionPipeline\n",
        "from PIL import Image, ImageDraw, ImageFilter, ImageFile\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "import cv2\n",
        "import diffusers\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from diffusers.models.attention_processor import (\n",
        "      AttnProcessor,\n",
        "      AttnProcessor2_0,\n",
        "  )\n",
        "\n",
        "from controlnet_aux.processor import Processor\n",
        "\n",
        "from importlib import reload\n",
        "import lpw_stable_diffusion\n",
        "import lpw_stable_diffusion_xl\n",
        "import ADetailer\n",
        "from safetensors_file import SafeTensorsFile\n",
        "\n",
        "import threading\n",
        "from uNet2DConditionModelFree import *\n",
        "import json\n",
        "\n",
        "from insightface.app import FaceAnalysis\n",
        "from insightface.utils import face_align\n",
        "\n",
        "from diffusers.models.attention_processor import AttnProcessor2_0\n",
        "from diffusers import AutoencoderKL, DEISMultistepScheduler, LMSDiscreteScheduler, KDPM2DiscreteScheduler, PNDMScheduler, DPMSolverMultistepScheduler,KDPM2AncestralDiscreteScheduler, UniPCMultistepScheduler, EulerDiscreteScheduler, DDPMScheduler, EulerAncestralDiscreteScheduler, DPMSolverSinglestepScheduler,DDIMScheduler, HeunDiscreteScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C_CCB-drC9m"
      },
      "outputs": [],
      "source": [
        "def path(a,ck=True,lora=False,emb=False,cont=False,vae=False):\n",
        "  if(ck):\n",
        "    return model_path+\"/\"+a\n",
        "  else:\n",
        "   if lora:\n",
        "    return lora_path+\"/\"+a\n",
        "   elif cont:\n",
        "    return controlnet_path+\"/\"+a\n",
        "   elif emb:\n",
        "    return embedding_path+\"/\"+a\n",
        "   else:\n",
        "    return vaes_path+\"/\"+a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA4GBIpvxjgB"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "def fresh():\n",
        "  gc.collect()\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lOS8jQnMXBQ"
      },
      "outputs": [],
      "source": [
        "def getLoras(path):\n",
        "    loras = {}\n",
        "\n",
        "    for path, sub_folder, files in os.walk(path):\n",
        "        for folder in sub_folder:\n",
        "            if not(\".ipynb_checkpoints\" in folder):\n",
        "               loras[folder] = {}\n",
        "        fol=len(loras.keys())\n",
        "\n",
        "        files.sort()\n",
        "        for elem in files:\n",
        "           if elem.endswith(\"safetensors\") or elem.endswith(\"pt\"):\n",
        "\n",
        "              if fol>0:\n",
        "                loras[path.split(\"/\")[-1]][elem.split(\".\")[0]] = {\"name_file\":elem, \"path\":path}\n",
        "              else:\n",
        "                loras[elem.split(\".\")[0]] = {\"name_file\":elem, \"path\":path}\n",
        "\n",
        "    return loras\n",
        "\n",
        "def get_tokens(input_file:str) -> int:\n",
        "    s=SafeTensorsFile.open_file(input_file)\n",
        "    js=s.get_header()\n",
        "    if js is None: return -1\n",
        "\n",
        "    tags = []\n",
        "    if \"__metadata__\" in js.keys():\n",
        "      if \"ss_tag_frequency\" in js[\"__metadata__\"].keys():\n",
        "        for elem in json.loads(js[\"__metadata__\"][\"ss_tag_frequency\"]):\n",
        "          tags += ([str(x) for x in (json.loads(js[\"__metadata__\"][\"ss_tag_frequency\"])[elem].keys())])\n",
        "    return tags[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slZ72_1CVxZH"
      },
      "outputs": [],
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfl9NmT6YXWd"
      },
      "outputs": [],
      "source": [
        "def prepareImage(path, app0):\n",
        "\n",
        "  #app0 = FaceAnalysis(name=\"antelopev2\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "  app0.prepare(ctx_id=0, det_size=(640, 640))\n",
        "\n",
        "  image = cv2.imread(path)#\"A.jpeg\")\n",
        "  im = image\n",
        "  faceid_embeds=None\n",
        "  for size in [(size,size) for size in range(768,128,-64)]:\n",
        "    app0.det_model.det_size=size\n",
        "    print(\"detection size:\"+str(size))\n",
        "    im = image\n",
        "    for sizei in [(sizex,sizey)  for sizex in range(640, 448, -64) for sizey in range(640,448,-64)]:\n",
        "\n",
        "     im = cv2.resize(im, sizei)\n",
        "     faces = app0.get(im)\n",
        "     if len(faces)>0:\n",
        "\n",
        "\n",
        "\n",
        "      print(\"face reconize:\"+str(sizei))\n",
        "      faceid_embeds =(torch.from_numpy(faces[0].normed_embedding).unsqueeze(0))\n",
        "      face_image = (face_align.norm_crop(im, landmark=faces[0].kps, image_size=512)) # you can also segment the fac\n",
        "\n",
        "      break\n",
        "\n",
        "\n",
        "\n",
        "     #faces = app0.get(im)\n",
        "\n",
        "    if(faceid_embeds!=None):\n",
        "      break\n",
        "  print(size, im.shape)\n",
        "  return faceid_embeds , face_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "half_value = True\n",
        "vae_tiny = None\n",
        "sdxl = False\n",
        "\n",
        "def vae_load():\n",
        "  global vae_tiny\n",
        "  vae_tiny_t = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\" if not(sdxl) else \"madebyollin/taesdxl\", torch_dtype=torch.float16 if half_value else torch.float32)\n",
        "  vae_tiny = vae_tiny_t.to(\"cuda\")\n",
        "  del vae_tiny_t\n",
        "\n",
        "last_thread = None"
      ],
      "metadata": {
        "id": "Jn9hFb-MPrFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4s2O5W6OznZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_pipeline_embeds(pipeline, prompt, negative_prompt, device):\n",
        "\n",
        "    max_length = pipeline.tokenizer.model_max_length\n",
        "\n",
        "    print(max_length)\n",
        "\n",
        "    # simple way to determine length of tokens\n",
        "    count_prompt = (pipeline.tokenizer(prompt, return_tensors=\"pt\", truncation=False).input_ids).shape[-1]\n",
        "    count_negative_prompt = (pipeline.tokenizer(negative_prompt, return_tensors=\"pt\", truncation=False).input_ids).shape[-1]\n",
        "\n",
        "\n",
        "\n",
        "    # create the tensor based on which prompt is longer\n",
        "    if count_prompt >= count_negative_prompt:\n",
        "        input_ids = pipeline.tokenizer(prompt, return_tensors=\"pt\", truncation=False).input_ids.to(device)\n",
        "        shape_max_length = input_ids.shape[-1]\n",
        "        negative_ids = pipeline.tokenizer(negative_prompt, truncation=False, padding=\"max_length\",\n",
        "                                          max_length=shape_max_length, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "        print(\"  length of prompt:\\t\"+str(shape_max_length))\n",
        "\n",
        "    else:\n",
        "        negative_ids = pipeline.tokenizer(negative_prompt, return_tensors=\"pt\", truncation=False).input_ids.to(device)\n",
        "        shape_max_length = negative_ids.shape[-1]\n",
        "        input_ids = pipeline.tokenizer(prompt, return_tensors=\"pt\", truncation=False, padding=\"max_length\",\n",
        "                                       max_length=shape_max_length).input_ids.to(device)\n",
        "        print(\"  length of prompt:\\t\"+str(shape_max_length))\n",
        "\n",
        "    concat_embeds = []\n",
        "    neg_embeds = []\n",
        "    for i in range(0, shape_max_length, max_length):\n",
        "        concat_embeds.append(pipeline.text_encoder(input_ids[:, i: i + max_length])[0])\n",
        "        neg_embeds.append(pipeline.text_encoder(negative_ids[:, i: i + max_length])[0])\n",
        "\n",
        "    return torch.cat(concat_embeds, dim=1), torch.cat(neg_embeds, dim=1)\n",
        "\n",
        "def modifyScheduler(pipeline,scheduler):\n",
        "   if(scheduler=='DDPMScheduler'):\n",
        "     return DDPMScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   elif(scheduler=='DEISMultistepScheduler'):\n",
        "     return DEISMultistepScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   elif(scheduler=='DPMSolverMultistepScheduler'):\n",
        "     return DPMSolverMultistepScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   elif(scheduler=='DPMSolverSinglestepScheduler'):\n",
        "     return DPMSolverSinglestepScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   elif(scheduler=='EulerAncestralDiscreteScheduler'):\n",
        "     return EulerAncestralDiscreteScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   elif(scheduler=='EulerDiscreteScheduler'):\n",
        "     return EulerDiscreteScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   elif(scheduler=='HeunDiscreteScheduler'):\n",
        "     return HeunDiscreteScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   elif(scheduler=='KDPM2AncestralDiscreteScheduler'):\n",
        "     return KDPM2AncestralDiscreteScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   elif(scheduler=='KDPM2DiscreteScheduler'):\n",
        "     return KDPM2DiscreteScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   elif(scheduler=='LMSDiscreteScheduler'):\n",
        "     return LMSDiscreteScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   elif(scheduler=='PNDMScheduler'):\n",
        "     return PNDMScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "   else:\n",
        "     return UniPCMultistepScheduler.from_config(pipeline[\"model\"].scheduler.config)\n",
        "\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "\n",
        "    ll = len(imgs)\n",
        "\n",
        "    while(rows*cols<ll):\n",
        "      rows+=1\n",
        "\n",
        "    grid = Image.new('RGBA', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "\n",
        "def generate(pipe, args):\n",
        "\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  image = []\n",
        "  seeds = 0\n",
        "  if args[\"seed\"]==None:\n",
        "    seeds = [random.randint(-int(sys.maxsize),int(sys.maxsize))]\n",
        "  else:\n",
        "    seeds = args[\"seed\"]\n",
        "\n",
        "  ImaTot = []\n",
        "  width = int(args[\"width\"] //8)*8\n",
        "  height= int(args[\"height\"] //8)*8\n",
        "\n",
        "\n",
        "\n",
        "  for _ in range(args[\"batch_number\"]):\n",
        "    generator = torch.Generator(device).manual_seed(seeds[-1])\n",
        "    seeds.append(seeds[-1]+1)\n",
        "    for (idE,pr) in enumerate([[args[\"prompt\"]]]):\n",
        "      pr_str  = (''.join(pr))\n",
        "      neg_str = (''.join([[args[\"negPrompt\"]]][idE]))\n",
        "      step_str = ''.join([str(stp) for stp in [args[\"step\"]]])\n",
        "      print(\"Image generator:\\n  positive prompt:\\t\"+pr_str+\"\\n  negative prompt:\\t\"+neg_str+\"\\n  number of steps:\\t\"+step_str+\"\\n  size image:\\t\\t(\"+str(width)+\",\"+str(height)+\")\\n  seed value:\\t\\t\"+str(seeds[-1])+\"\\n  strength generator:\\t\"+str(args[\"strength\"]))\n",
        "      image = []\n",
        "      for step in [args[\"step\"]]:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        generator = torch.Generator(device).manual_seed(seeds[-1])\n",
        "\n",
        "        pr = pr * (args[\"batch_size\"]  if not(\"sdxl\" in args ) else 1)\n",
        "        negPr = \"\"\n",
        "        if not(\"sdxl\" in args ):\n",
        "          negPr = [[args[\"negPrompt\"]]][idE] * args[\"batch_size\"]\n",
        "        else:\n",
        "          negPr = [[args[\"negPrompt\"]]][idE]\n",
        "\n",
        "        init_image = args[\"init_image\"].convert(\"RGB\")\n",
        "\n",
        "\n",
        "        mask_image = args[\"mask_image\"].convert(\"RGB\")\n",
        "\n",
        "\n",
        "        if \"cb\" not in args.keys():\n",
        "          args[\"cb\"] = None\n",
        "\n",
        "        with torch.inference_mode():\n",
        "          if \"image\" in args[\"Pp\"]:\n",
        "              image.append(pipe.img2img(prompt=pr, negative_prompt=negPr, guidance_scale=args[\"strength\"], generator=generator, num_inference_steps=step,\n",
        "                                        width=width, height=height, image=init_image, strength=args[\"imageStrength\"],\n",
        "                                        max_embeddings_multiples=args[\"mxl\"], callback=args[\"cb\"], **args[\"sdxl\"]).images)\n",
        "          elif \"text\" in args[\"Pp\"]:\n",
        "              image.append(pipe.text2img(prompt=pr, negative_prompt=negPr, guidance_scale=args[\"strength\"], generator=generator, num_inference_steps=step,\n",
        "                                         width=width, height=height, max_embeddings_multiples=args[\"mxl\"], callback=args[\"cb\"],\n",
        "                                         controlnet_conditioning_scale=args[\"controlnet_strenght\"],\n",
        "                                         image_controlnet=args[\"controlnet_image\"], control_guidance_start=args[\"start_cnt\"],\n",
        "                                         control_guidance_end=args[\"end_cnt\"], **args[\"sdxl\"]).images)\n",
        "          else:\n",
        "              print(\"inpaint\")\n",
        "              display(mask_image)\n",
        "              image.append(pipe.inpaint(prompt=pr, negative_prompt=negPr, guidance_scale=args[\"strength\"], generator=generator, num_inference_steps=step,\n",
        "                                        width=width, height=height, image=(init_image), mask_image=mask_image,\n",
        "                                        strength=args[\"imageStrength\"], max_embeddings_multiples=args[\"mxl\"],\n",
        "                                        callback=args[\"cb\"], **args[\"sdxl\"]).images)\n",
        "\n",
        "      image = [imm for im in image for imm in im]\n",
        "\n",
        "      if(args[\"ip_adap_en\"]):\n",
        "\n",
        "\n",
        "        ima_temp = []\n",
        "        for ima in image:\n",
        "          ima_temp.append(args[\"model_face\"].generate(prompt=pr[0], negative_prompt=negPr[0], image=ima, strength=args[\"ip_adap_strength_ip\"],\n",
        "                                         faceid_embeds=args[\"ip_adap_faceid_embeds\"], face_image=args[\"ip_adap_face_image\"],\n",
        "                                         shortcut=args[\"ip_adap_v2\"], s_scale=args[\"ip_adap_s_scale\"], scale=args[\"ip_adap_scale\"],\n",
        "                                         num_samples=args[\"batch_size\"], width=width, height=height,\n",
        "                                         guidance_scale=args[\"strength\"], noise=args[\"ip_adap_noise\"], cross_attention_kwargs={\"scale\": 1.0},\n",
        "                                         num_inference_steps=args[\"ip_adap_steps\"], seed=seeds[-1], callback=args[\"cbf\"], callback_steps=1)[0])\n",
        "\n",
        "        ImaTot = ImaTot + ima_temp\n",
        "      else:\n",
        "        ImaTot = ImaTot + image\n",
        "\n",
        "      for i,_ in enumerate(ImaTot):\n",
        "\n",
        "          metadata = PngInfo()\n",
        "          metadata.add_text(\"positive_prompt\",  (''.join(pr)))\n",
        "          metadata.add_text(\"negative_prompt\", (''.join(negPr)))\n",
        "          metadata.add_text(\"seed\", str(seeds[len(seeds)-i-1]))\n",
        "          metadata.add_text(\"gs\", str(args[\"strength\"]))\n",
        "          metadata.add_text(\"step\", str([args[\"step\"]][i%(len([args[\"step\"]]))]))\n",
        "          metadata.add_text(\"width\", str(args[\"width\"]))\n",
        "          metadata.add_text(\"height\", str(args[\"height\"]))\n",
        "\n",
        "\n",
        "          ImaTot[i].save(\"NewPath.png\", pnginfo=metadata)\n",
        "\n",
        "          ImaTot[i] = Image.open(\"NewPath.png\")\n",
        "\n",
        "          !rm \"./NewPath.png\"\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  return ImaTot\n",
        "\n",
        "\n",
        "def deepp_ins(step, pred_original_sample, pipe):\n",
        "    global hbox, vae_tiny, out_preview\n",
        "    with torch.no_grad():\n",
        "\n",
        "        if half_value:\n",
        "          pred_original_sample = pred_original_sample.half()\n",
        "\n",
        "        imageT = vae_tiny.decode(pred_original_sample).sample\n",
        "\n",
        "        imageT = (imageT / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "        # # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n",
        "        imageT1 = imageT.cpu().permute(0, 2, 3, 1).float().numpy()\n",
        "\n",
        "        # # convert to PIL Images\n",
        "        image = pipe.numpy_to_pil(imageT1)\n",
        "        # # do something with the Images\n",
        "        with out_preview:\n",
        "             clear_output(wait=True)\n",
        "             for img in image:\n",
        "                 display(img)\n",
        "        hbox.children = [out_preview]\n",
        "        del image\n",
        "        del imageT\n",
        "        del imageT1\n",
        "        gc.collect()\n",
        "        fresh()\n",
        "        return\n",
        "\n",
        "def deepp_ins_lat(step, latent, pipe):\n",
        "    global hbox, vae_tiny, out_preview\n",
        "    with torch.no_grad():\n",
        "\n",
        "\n",
        "        imageT = vae_tiny.decode(latent).sample\n",
        "\n",
        "\n",
        "        if half_value:\n",
        "          imageT = imageT.half()\n",
        "\n",
        "        imageT = (imageT / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "        # # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n",
        "        imageT1 = imageT.cpu().permute(0, 2, 3, 1).float().numpy()\n",
        "\n",
        "        # # convert to PIL Images\n",
        "        image = pipe.numpy_to_pil(imageT1)\n",
        "        # # do something with the Images\n",
        "        with out_preview:\n",
        "             clear_output(wait=True)\n",
        "             for img in image:\n",
        "                 if img.width>1025 or img.height>1025:\n",
        "                  w,h = img.size\n",
        "\n",
        "                  r = w/h\n",
        "                  w1,h1=(w,h)\n",
        "                  if img.width>img.height:\n",
        "                    w1=1024\n",
        "                    h1=w1/r\n",
        "                  else:\n",
        "                    h1=1024\n",
        "                    w1=r*h1\n",
        "                  img  = img.resize((int(w1),int(h1)))\n",
        "                 display(img)\n",
        "        hbox.children = [out_preview]\n",
        "        del image\n",
        "        del imageT\n",
        "        del imageT1\n",
        "        gc.collect()\n",
        "        fresh()\n",
        "        return\n",
        "\n",
        "def cb(step, t, latents, pred_original_sample):\n",
        "    global preview, last_thread\n",
        "\n",
        "    if \"enable_freeU\" in locals():\n",
        "        if strength_freeU < 1 and step >= int(step * denoise * strength_freeU):\n",
        "                pipe[\"model\"].unet.freeu.ones()\n",
        "    if preview and ((step+1)%10==0 or step==0):\n",
        "        if last_thread != None:\n",
        "            last_thread.exit()\n",
        "            del last_thread\n",
        "            last_thread = None\n",
        "        last_thread = threading.Thread(target=deepp_ins, args=(step, pred_original_sample, pipe[\"model\"])).start()\n",
        "\n",
        "def cbl(step, t, latents):\n",
        "    global preview, last_thread\n",
        "\n",
        "    if \"enable_freeU\" in locals():\n",
        "        if strength_freeU < 1 and step >= int(step * denoise * strength_freeU):\n",
        "                pipe[\"model\"].unet.freeu.ones()\n",
        "    if preview and ((step+1)%10==0 or step==0):\n",
        "        if last_thread != None:\n",
        "            last_thread.exit()\n",
        "            del last_thread\n",
        "            last_thread = None\n",
        "        last_thread = threading.Thread(target=deepp_ins_lat, args=(step, latents, pipe[\"model\"])).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28QVFA-8uVEL"
      },
      "outputs": [],
      "source": [
        "pipe= {\"model\":None, \"clip\":None, \"clip_lora\":None, \"model_path\":None, \"model_config\":None, \"model_vae\":None, \"model_face\":None}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DIouIUpSpz4"
      },
      "source": [
        "#Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y27-68-IpOfM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Path select or Refresh models path\n",
        "model_path = \"/models/checkpoints\" #@param {type:\"string\"}\n",
        "lora_path = \"/models/loras\"#@param {type:\"string\"}\n",
        "embedding_path = \"/models/embeddings\"#@param {type:\"string\"}\n",
        "vaes_path = \"/models/vae\"#@param {type:\"string\"}\n",
        "ipAd_path = \"/models/ip_ada/\"#@param {type:\"string\"}\n",
        "\n",
        "\n",
        "model_path = COMF +\"/\"+model_path\n",
        "lora_path = COMF +\"/\"+lora_path\n",
        "embedding_path = COMF +\"/\"+embedding_path\n",
        "vaes_path = COMF +\"/\"+vaes_path\n",
        "ipAd_path = COMF +\"/\"+ipAd_path\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "if not(os.path.exists(model_path)):\n",
        "  Path(model_path).mkdir(parents=True, exist_ok=True)\n",
        "if not(os.path.exists(lora_path)):\n",
        "  Path(lora_path).mkdir(parents=True, exist_ok=True)\n",
        "if not(os.path.exists(embedding_path)):\n",
        "  Path(embedding_path).mkdir(parents=True, exist_ok=True)\n",
        "if not(os.path.exists(vaes_path)):\n",
        "  Path(vaes_path).mkdir(parents=True, exist_ok=True)\n",
        "if not(os.path.exists(vaes_path)):\n",
        "  Path(vaes_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "models = [i for i in os.listdir(model_path) if i.endswith(\"safetensors\") or i.endswith(\"ckpt\")]\n",
        "loras = getLoras(lora_path)\n",
        "embedding = [i for i in os.listdir(embedding_path) if i.endswith(\"safetensors\") or i.endswith(\"ckpt\") or i.endswith(\".pt\")]\n",
        "configs = [i for i in os.listdir(model_path) if i.endswith(\"yaml\")]\n",
        "vaes = [i for i in os.listdir(vaes_path) if i.endswith(\"safetensors\")]\n",
        "# ipAd = [i for i in os.listdir(ipAd_path) if i.endswith(\"safetensors\") or i.endswith(\".pt\")]\n",
        "controlnets = [\"canny\", \"openpose\",\"depth\",\"lineart\",\"normalbae\"]\n",
        "\n",
        "models.sort()\n",
        "embedding.sort()\n",
        "configs.sort()\n",
        "vaes.sort()\n",
        "# ipAd.sort()\n",
        "configs = [\"default\"]+configs\n",
        "vaes = [\"default\"]+vaes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f975r5YmKahU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download model\n",
        "import subprocess\n",
        "\n",
        "name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "url= '' #@param{type:'string'}\n",
        "\n",
        "tipo = \"Checkpoint\" #@param [\"Lora\",\"Checkpoint\",\"Embedding\",\"Vae\"]\n",
        "token = \"\" #@param{type:\"string\"}\n",
        "url += (\"&token=\"+token if len(token)>0 else \"\")\n",
        "\n",
        "if \"Lora\" in tipo and url != \"\":\n",
        "\n",
        "  !/usr/bin/wget -c {\"\\\"\"+url+\"\\\"\"} -O {\"\\'\"+lora_path+'/'+name+\".safetensors\\'\"}\n",
        "if \"Checkpoint\" in tipo and url != \"\":\n",
        "  !wget -P -c {\"\\\"\"+url+\"\\\"\"} -O {\"\\'\"+model_path+'/'+name+\".safetensors\\'\"}\n",
        "\n",
        "if \"Embedding\" in tipo and url != \"\":\n",
        "  !wget -P -c {\"\\\"\"+url+\"\\\"\"} -O {\"\\'\"+embedding_path+'/'+name+\".safetensors\\'\"}\n",
        "\n",
        "\n",
        "if \"Vae\" in tipo and url != \"\":\n",
        "  !wget -P -c {\"\\\"\"+url+\"\\\"\"} -O {\"\\'\"+vaes_path+'/'+name+\".safetensors\\'\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJRYFnyZoyaF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Model Selector\n",
        "import copy\n",
        "import importlib\n",
        "importlib.reload(lpw_stable_diffusion)\n",
        "importlib.reload(lpw_stable_diffusion_xl)\n",
        "\n",
        "models_pick = widgets.Dropdown(options=models)\n",
        "model_label = widgets.Label(\"Model:\")\n",
        "\n",
        "\n",
        "config_pick  = widgets.Dropdown(options=configs)\n",
        "config_label = widgets.Label(\"Config:\")\n",
        "\n",
        "vae_pick  = widgets.Dropdown(options=vaes)\n",
        "vae_label = widgets.Label(\"Vae:\")\n",
        "button = widgets.Button(description=\"Load\", layout=widgets.Layout(width='200px'))\n",
        "\n",
        "\n",
        "def f(a,b,c):\n",
        "    print((a,b,c))\n",
        "\n",
        "ui = widgets.VBox([widgets.HBox([model_label,widgets.Box(layout=widgets.Layout(display='flex',flex_flow='row',align_items='stretch',width='2px')), models_pick]),\n",
        "                   widgets.HBox([config_label,config_pick]),\n",
        "                   widgets.HBox([vae_label,widgets.Box(layout=widgets.Layout(display='flex',flex_flow='row',align_items='stretch',width='15px'))   ,vae_pick]),\n",
        "                   widgets.HBox([widgets.Box(layout=widgets.Layout(display='flex',flex_flow='row',align_items='stretch',width='50px')), button], layout=widgets.Layout(display='flex',flex_flow='row',align_items='center', width=\"300px\"))])\n",
        "out = widgets.interactive_output(f, {'a': models_pick, 'b':config_pick, 'c':vae_pick})\n",
        "\n",
        "\n",
        "def loadModel(pipe,model_path, configuration, vae_path):\n",
        "    global vae_tiny, sdxl\n",
        "    device = \"cuda\"\n",
        "    fresh()\n",
        "    fresh()\n",
        "    fresh()\n",
        "    if \"model\" in pipe.keys():\n",
        "      if pipe[\"model\"] != None:\n",
        "        del pipe[\"model\"]\n",
        "        if \"lora\" in pipe.keys():\n",
        "          del pipe[\"lora\"]\n",
        "        del pipe\n",
        "        for _ in range(0,10):\n",
        "         fresh()\n",
        "        pipe= {\"model\":None, \"clip\":None, \"clip_lora\":None, \"model_path\":None, \"model_config\":None, \"model_vae\":None, \"model_face\":None}\n",
        "        for _ in range(0,10):\n",
        "          fresh()\n",
        "        return -1\n",
        "    if \"vae_tiny\" in locals() or \"vae_tiny\" in globals():\n",
        "      del vae_tiny\n",
        "\n",
        "    fresh()\n",
        "    fresh()\n",
        "    fresh()\n",
        "    pipe[\"model_path\"]=model_path\n",
        "    pipe[\"model_config\"]=configuration\n",
        "    pipe[\"model_vae\"]=vae_path\n",
        "    s=SafeTensorsFile.open_file(model_path)\n",
        "    js=s.get_header()\n",
        "    sdxl = False\n",
        "    test=False\n",
        "\n",
        "    if \"__metadata__\" in str(js) and str(js[\"__metadata__\"])!=\"{}\":\n",
        "      sdxl = \"pony\" in str(js[\"__metadata__\"]).lower() or (\"modelspec.architecture\" in str(js[\"__metadata__\"]) and str(js[\"__metadata__\"][\"modelspec.architecture\"])==\"stable-diffusion-xl-v1-base\")\n",
        "    else:\n",
        "      if \"conditioner.embedders.0\" in str(js) or \"conditioner.embedders.1\" in str(js):\n",
        "        sdxl=True\n",
        "    print(\"sdxl:\", sdxl)\n",
        "\n",
        "    try:\n",
        "      if sdxl:\n",
        "        pipe[\"model\"] = lpw_stable_diffusion_xl.SDXLLongPromptWeightingPipeline.from_single_file(model_path,use_safetensors=True, torch_dtype=torch.float16 if half_value else torch.float32, original_config_file=configuration)\n",
        "        pipe[\"model\"].vae.enable_vae_slicing = True\n",
        "\n",
        "      else:\n",
        "        pipe[\"model\"] = lpw_stable_diffusion.StableDiffusionLongPromptWeightingPipeline.from_single_file(model_path,use_safetensors=True, torch_dtype=torch.float16 if half_value else torch.float32, original_config_file=configuration)\n",
        "    except:\n",
        "      if sdxl:\n",
        "        sdxl = False\n",
        "        pipe[\"model\"] = lpw_stable_diffusion.StableDiffusionLongPromptWeightingPipeline.from_single_file(model_path,use_safetensors=True, torch_dtype=torch.float16 if half_value else torch.float32, original_config_file=configuration)\n",
        "\n",
        "      else:\n",
        "        sdxl = True\n",
        "        pipe[\"model\"] = lpw_stable_diffusion_xl.SDXLLongPromptWeightingPipeline.from_single_file(model_path,use_safetensors=True, torch_dtype=torch.float16 if half_value else torch.float32, original_config_file=configuration)\n",
        "        pipe[\"model\"].vae.enable_vae_slicing = True\n",
        "\n",
        "    if not(\"default\" in vae_path):\n",
        "      pipe[\"model\"].vae = AutoencoderKL.from_single_file(vae_path,use_safetensors=True, torch_dtype=torch.float16 if half_value else torch.float32)\n",
        "    pipe[\"model\"].enable_vae_tiling()\n",
        "    pipe[\"model\"].unet.set_attn_processor(AttnProcessor2_0())\n",
        "    pipe[\"model\"].unet.to(memory_format=torch.channels_last)\n",
        "\n",
        "    pipe[\"model\"] = pipe[\"model\"].to(device)\n",
        "\n",
        "    def dummy(images, **kwargs): return images, [False for x in images]\n",
        "    pipe[\"model\"].safety_checker = dummy\n",
        "    #pipe[\"model\"].enable_xformers_memory_efficient_attention()\n",
        "    for _ in range(0,20):\n",
        "      fresh()\n",
        "    print(\"loaded \"+model_path.split(\"/\")[-1])\n",
        "    #pipe[\"clip\"] = copy.deepcopy(pipe[\"model\"].text_encoder.text_model.encoder.layers)\n",
        "    return 0\n",
        "\n",
        "def on_button_clicked_ck(b,pipe):\n",
        "    global vae_tiny\n",
        "    button.disabled=True\n",
        "    print(\"Starting load, please wait\")\n",
        "\n",
        "    configuration = None\n",
        "\n",
        "    if config_pick.value==\"default\":\n",
        "      configuration = None\n",
        "    else:\n",
        "      configuration = path(config_pick.value)\n",
        "\n",
        "    r = loadModel(pipe ,path(models_pick.value), configuration, path(vae_pick.value,ck=False, vae=True))\n",
        "    if r==-1:\n",
        "        loadModel(pipe ,path(models_pick.value), configuration, path(vae_pick.value,ck=False, vae=True))\n",
        "\n",
        "    button.disabled=False\n",
        "\n",
        "button.on_click(lambda b: on_button_clicked_ck(b,pipe))\n",
        "\n",
        "display(ui, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mFYgmmYd4C79",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Lora Select\n",
        "from pathlib import Path\n",
        "\n",
        "pipe[\"model\"].enable_lora()\n",
        "\n",
        "def f(a):\n",
        "  print((a))\n",
        "\n",
        "lora_pick = widgets.Dropdown(options=loras)\n",
        "button1 = widgets.Button(description=\"Load\")\n",
        "button2 = widgets.Button(description=\"Unload\")\n",
        "button3 = widgets.Button(description=\"Norm W\")\n",
        "strengt = widgets.BoundedFloatText(\n",
        "    value=1.0,\n",
        "    min=-6.0,\n",
        "    max=6.0,\n",
        "    step=0.05,\n",
        "    description='Strength:',\n",
        "    disabled=False\n",
        ")\n",
        "ui = widgets.VBox([widgets.HBox([lora_pick]), widgets.HBox([strengt]), widgets.HBox([ button1, button2, button3])])\n",
        "out = widgets.interactive_output(f, {'a': lora_pick})\n",
        "lora_pick.ok=0\n",
        "\n",
        "if \"lora\" in pipe.keys():\n",
        "  print(\"Active lora: \",pipe[\"model\"].get_active_adapters(), pipe[\"lora\"])\n",
        "\n",
        "def getNames(v=0):\n",
        "\n",
        "  return [l if v==0 else ln for l,ln,st in pipe[\"lora\"]]\n",
        "\n",
        "def getStrengt():\n",
        "  return [float(st) for l,ln,st in pipe[\"lora\"]]\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    global pipe\n",
        "    button1.disabled=True\n",
        "    button2.disabled=True\n",
        "    button3.disabled=True\n",
        "    device = \"cuda\"\n",
        "\n",
        "    if \"lora\" not in pipe.keys():\n",
        "      pipe[\"lora\"]=[]\n",
        "\n",
        "\n",
        "    path_loraName = str(Path(lora_path+\"/\"+lora_pick.value[\"name_file\"]))\n",
        "    w_name = str(lora_pick.value[\"name_file\"]).replace(\"-\",\"\").replace(\"_\",\"\")\n",
        "    ad_name = w_name.split(\".\")[0]\n",
        "    #print(path_loraName, w_name, ad_name)\n",
        "\n",
        "    if(ad_name not in getNames()):\n",
        "      pipe[\"model\"].load_lora_weights(path_loraName, weight_name=w_name, adapter_name=ad_name)\n",
        "\n",
        "    print(\"loaded \"+(path_loraName)+\" strengt \"+str(strengt.value))\n",
        "    print(\"tokens: \"+str(get_tokens(lora_pick.value[\"path\"]+\"/\"+lora_pick.value[\"name_file\"])))\n",
        "    if(ad_name not in getNames()):\n",
        "      last_name= pipe[\"model\"].get_active_adapters()[-1]\n",
        "      pipe[\"lora\"].append([ad_name,last_name,float(strengt.value)])\n",
        "    else:\n",
        "      el = [[l,ln,s] for l,ln,s in pipe[\"lora\"] if l==ad_name][0]\n",
        "      pipe[\"lora\"][pipe[\"lora\"].index(el)] = [el[0],el[1],float(strengt.value)]\n",
        "    print(getNames(),getStrengt())\n",
        "\n",
        "    pipe[\"model\"].set_adapters(getNames(1),adapter_weights=getStrengt())\n",
        "\n",
        "    print(\"Active lora: \",pipe[\"model\"].get_active_adapters(), pipe[\"lora\"])\n",
        "    button1.disabled=False\n",
        "    button2.disabled=False\n",
        "    button3.disabled=False\n",
        "\n",
        "def on_button_clicked2(b):\n",
        "    global pipe\n",
        "    button2.disabled=True\n",
        "    button1.disabled=True\n",
        "    button3.disabled=True\n",
        "\n",
        "    if \"lora\" not in pipe.keys():\n",
        "      pipe[\"lora\"]=[]\n",
        "\n",
        "    if lora_pick.value[\"name_file\"].split(\".\")[0] in getNames():\n",
        "      del pipe[\"lora\"][pipe[\"lora\"].index([[l,s] for l,s in pipe[\"lora\"] if l==lora_pick.value[\"name_file\"].split(\".\")[0]][0])]\n",
        "      pipe[\"model\"].set_adapters(getNames(),adapter_weights=getStrengt())\n",
        "      print(\"Lora unloaded \")\n",
        "\n",
        "    button1.disabled=False\n",
        "    button2.disabled=False\n",
        "    button3.disabled=False\n",
        "\n",
        "def normalize(b):\n",
        "  global pipe\n",
        "  nor = sum(getStrengt())\n",
        "  nStrengt = []\n",
        "  for i,(l,s) in enumerate(pipe[\"lora\"]):\n",
        "    nStrengt.append(s/nor)\n",
        "    pipe[\"lora\"][i] = [l,s/nor]\n",
        "  pipe[\"model\"].set_adapters(getNames(), nStrengt)\n",
        "\n",
        "  print(\"Active lora: \", pipe[\"lora\"])\n",
        "\n",
        "button1.on_click(lambda b: on_button_clicked(b))\n",
        "button2.on_click(lambda b: on_button_clicked2(b))\n",
        "button3.on_click(lambda b: normalize(b))\n",
        "\n",
        "last_change = {}\n",
        "def on_change(change):\n",
        "    global last_change\n",
        "\n",
        "    if \"lora\" not in pipe.keys():\n",
        "      pipe[\"lora\"]=[]\n",
        "    if change.name==\"index\":\n",
        "      print(\"Active lora: \",pipe[\"model\"].get_active_adapters(), pipe[\"lora\"])\n",
        "    def nothing(c):\n",
        "        pass\n",
        "    def isFolder(val):\n",
        "       return not(\"name_file\" in val.keys())\n",
        "\n",
        "    if change['type'] == 'change' and change['name'] == 'value' and lora_pick.ok==0:\n",
        "\n",
        "        lora_pick.ok=1\n",
        "\n",
        "\n",
        "        if \"..\" not in lora_pick.options and isFolder(lora_pick.value):\n",
        "\n",
        "            new_dict = {\"..\":{}}\n",
        "            new_dict.update(change.new)\n",
        "            lora_pick.options = new_dict\n",
        "            lora_pick.index = 1\n",
        "\n",
        "            lora_pick.ok=2\n",
        "\n",
        "\n",
        "    elif change['type'] == 'change' and change['name'] == 'value' and lora_pick.value=={} and lora_pick.ok==2:\n",
        "        lora_pick.ok=3\n",
        "        lora_pick.observe(nothing, names=[\"value\"])\n",
        "        lora_pick.options = loras\n",
        "\n",
        "        lora_pick.observe(on_change, names=[\"value\"])\n",
        "\n",
        "        lora_pick.ok=0\n",
        "\n",
        "\n",
        "lora_pick.observe(on_change)\n",
        "\n",
        "display(ui, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gqoRqRhD-Yvd"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Embedding Select\n",
        "\n",
        "def f(a):\n",
        "  print((a))\n",
        "\n",
        "emb_pick = widgets.Dropdown(options=embedding)\n",
        "button13 = widgets.Button(description=\"Load\")\n",
        "strengt1 = widgets.BoundedFloatText(\n",
        "    value=1.0,\n",
        "    min=0.0,\n",
        "    max=2.0,\n",
        "    step=0.05,\n",
        "    description='Strength:',\n",
        "    disabled=False\n",
        ")\n",
        "ui = widgets.VBox([widgets.HBox([emb_pick]), widgets.HBox([ button13])])\n",
        "out = widgets.interactive_output(f, {'a': emb_pick})\n",
        "\n",
        "def butt(b,pipe):\n",
        "  pipe[\"model\"].load_textual_inversion(embedding_path, weight_name=emb_pick.value, local_files_only=True)\n",
        "\n",
        "button13.on_click(lambda b:butt(b,pipe))\n",
        "display(ui,out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "T3mVn42UCK_f"
      },
      "outputs": [],
      "source": [
        "#@title Clip Skip\n",
        "#@markdown work with base model and lora on base model\n",
        "clip_skip = 2 #@param {type:\"integer\"}\n",
        "\n",
        "try:\n",
        "  if \"clip_layers\" in locals():\n",
        "    del clip_layers\n",
        "finally:\n",
        "  print()\n",
        "\n",
        "clip_layers = None\n",
        "\n",
        "if \"clip_lora\" in pipe.keys() and pipe[\"clip_lora\"]!=None:\n",
        "  clip_layers = pipe[\"clip_lora\"]\n",
        "else:\n",
        "  clip_layers = pipe[\"clip\"]\n",
        "if clip_skip > 0:\n",
        "  del pipe[\"model\"].text_encoder.text_model.encoder.layers\n",
        "  pipe[\"model\"].text_encoder.text_model.encoder.layers = copy.deepcopy(clip_layers[:-clip_skip])\n",
        "else:\n",
        "  del pipe[\"model\"].text_encoder.text_model.encoder.layers\n",
        "  pipe[\"model\"].text_encoder.text_model.encoder.layers = copy.deepcopy(clip_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCJ5b_9LVBuw"
      },
      "source": [
        "## ControlNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChiYP4Y4VAt1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ControlNet\n",
        "\n",
        "\n",
        "def f(a):\n",
        "  print((a))\n",
        "\n",
        "def rename(val):\n",
        "    if \"depth\" in val:\n",
        "        return \"control_v11f1p_sd15_depth\"\n",
        "    else:\n",
        "        return \"control_v11p_sd15_\"+val\n",
        "\n",
        "cnt_pick = widgets.Dropdown(options=controlnets)\n",
        "button14 = widgets.Button(description=\"Load\")\n",
        "strengt15 = widgets.BoundedFloatText(\n",
        "    value=1.0,\n",
        "    min=0.0,\n",
        "    max=2.0,\n",
        "    step=0.05,\n",
        "    description='Strength:',\n",
        "    disabled=False\n",
        ")\n",
        "ui = widgets.VBox([widgets.HBox([cnt_pick]), widgets.HBox([ button14])])\n",
        "out = widgets.interactive_output(f, {'a': cnt_pick})\n",
        "\n",
        "def butt(b,pipe):\n",
        "  controlnet = lpw_stable_diffusion.ControlNetModel.from_pretrained(\"lllyasviel/\"+rename(cnt_pick.value), torch_dtype=torch.float16)\n",
        "  pipe[\"model\"].set_controlnet(controlnet)\n",
        "\n",
        "button14.on_click(lambda b:butt(b,pipe))\n",
        "display(ui)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N9otBmKZVO4f"
      },
      "outputs": [],
      "source": [
        "#@title #Load image (ControlNet)\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import PIL\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "\n",
        "loadImage = False\n",
        "image_controlnet = None\n",
        "\n",
        "\n",
        "display(\"Load Image:\")\n",
        "Up = files.upload()\n",
        "for k, v in Up.items():\n",
        "    image_controlnet=Image.open(k)\n",
        "\n",
        "image_controlnet.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Ct8kO4TzTe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Preprocess ControlNet\n",
        "import cv2\n",
        "import numpy\n",
        "\n",
        "\n",
        "precprocess = \"openpose_full\" #@param [\"canny\", \"openpose\", \"openpose_full\", \"normal_bae\", \"depth_leres\", \"depth_leres++\"]\n",
        "size_precision = 768 #@param [512, 640, 768, 960]\n",
        "\n",
        "preprocessor =  Processor(precprocess)\n",
        "open_cv_image = numpy.array(image_controlnet)\n",
        "# Convert RGB to BGR\n",
        "open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
        "image_controlnet = preprocessor(open_cv_image, detect_resolution=size_precision)\n",
        "if \"canny\" in precprocess:\n",
        "  image_controlnet=Image.fromarray(image_controlnet)\n",
        "\n",
        "image_controlnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxw_EHEFtB3g"
      },
      "source": [
        "##Image2Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "T-BvCKOiLqNh"
      },
      "outputs": [],
      "source": [
        "#@title #Load image (img2img)\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import PIL\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "\n",
        "#@markdown #Load image from upload file\n",
        "#@markdown Size of image\n",
        "\n",
        "#@title Dimensioni\n",
        "def sizeR(size):\n",
        "  if size%64<=16:\n",
        "    return size\n",
        "  else:\n",
        "    return (size//64)*64+64\n",
        "\n",
        "loadImage = False\n",
        "mask = Image.new('RGBA', (512, 512),  (255, 255, 255))\n",
        "initt = None\n",
        "url = None\n",
        "\n",
        "\n",
        "display(\"Load Image:\")\n",
        "Up = files.upload()\n",
        "for k, v in Up.items():\n",
        "    initt=Image.open(k)\n",
        "display(\"Load Mask:\")\n",
        "Up = files.upload()\n",
        "for k, v in Up.items():\n",
        "      mask=Image.open(k)\n",
        "\n",
        "\n",
        "print(initt.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oXkyzslL5bzk"
      },
      "outputs": [],
      "source": [
        "#@title Resize image\n",
        "\n",
        "\n",
        "width = 576 #@param Integer\n",
        "height = 1280 #@param Integer\n",
        "mSize = width\n",
        "\n",
        "rat = mSize\n",
        "\n",
        "initt.thumbnail((rat,rat))\n",
        "w,h= (width,height)\n",
        "\n",
        "w = sizeR(w)\n",
        "h = sizeR(h)\n",
        "initt=initt.resize((w,h))\n",
        "mask = mask.resize((w,h))\n",
        "\n",
        "display(image_grid([initt,mask],1,2))\n",
        "\n",
        "def loadFromUrl():\n",
        "  url=\"https://www.google.com/images/branding/googlelogo/2x/googlelogo_light_color_272x92dp.png\"\n",
        "  response = requests.get(url)\n",
        "  initt = Image.open(BytesIO(response.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6zeKjUwOx5s"
      },
      "outputs": [],
      "source": [
        "#@title Drawing mask\n",
        "#@markdown Only for PC\n",
        "from IPython.display import HTML\n",
        "from google.colab.output import eval_js\n",
        "import PIL.ImageOps\n",
        "from base64 import b64decode\n",
        "from base64 import b64encode\n",
        "\n",
        "canvas_html = \"\"\"\n",
        "<img src='%s' width=%d  height=%d' >\n",
        "<canvas width=%d height=%d style='position:absolute; top:0; left:0; z-index:2'></canvas>\n",
        "\n",
        "<button>Finish</button>\n",
        "<script>\n",
        "var canvas = document.querySelector('canvas')\n",
        "var ctx = canvas.getContext('2d')\n",
        "ctx.lineWidth = %d\n",
        "var button = document.querySelector('button')\n",
        "var mouse = {x: 0, y: 0}\n",
        "canvas.addEventListener('mousemove', function(e) {\n",
        "  mouse.x = e.pageX - this.offsetLeft\n",
        "  mouse.y = e.pageY - this.offsetTop\n",
        "})\n",
        "canvas.onmousedown = ()=>{\n",
        "  ctx.beginPath()\n",
        "  ctx.moveTo(mouse.x, mouse.y)\n",
        "  canvas.addEventListener('mousemove', onPaint)\n",
        "}\n",
        "canvas.onmouseup = ()=>{\n",
        "  canvas.removeEventListener('mousemove', onPaint)\n",
        "}\n",
        "var onPaint = ()=>{\n",
        "  ctx.lineTo(mouse.x, mouse.y)\n",
        "  ctx.stroke()\n",
        "}\n",
        "var data = new Promise(resolve=>{\n",
        "  button.onclick = ()=>{\n",
        "    resolve(canvas.toDataURL('image/png'))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def draw(filename='drawing.png', w=400, h=200, line_width=1):\n",
        "\n",
        "  mime_type = None\n",
        "  initt.save(\"/content/my_image.png\")\n",
        "  path_to_image = \"/content/my_image.png\"\n",
        "\n",
        "  mime_type = 'image/png'\n",
        "\n",
        "  img = open(path_to_image, 'rb').read()\n",
        "  data_url = 'data:image/jpeg;base64,' + b64encode(img).decode()\n",
        "\n",
        "  display(HTML(canvas_html % (data_url, w, h, w, h, line_width)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "\n",
        "  return len(binary)\n",
        "\n",
        "\n",
        "w1,h1 = initt.size\n",
        "draw(\"my_drawing.png\",w1,h1,min(w1,h1)/10)\n",
        "mask = Image.open(\"my_drawing.png\")\n",
        "\n",
        "newImage = []\n",
        "for item in mask.getdata():\n",
        "    if item[3] <=10:\n",
        "        newImage.append((0,0,0, 255))\n",
        "    else:\n",
        "        newImage.append((255,255,255,255))\n",
        "\n",
        "mask.putdata(newImage)\n",
        "\n",
        "display(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Bj7PZcaaJt9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title prepare mask\n",
        "#@markdown lunch to synchronize the image\n",
        "import PIL.ImageOps\n",
        "\n",
        "\n",
        "\n",
        "if mask.mode == 'RGBA':\n",
        "    r,g,b,a = mask.split()\n",
        "    rgb_image = Image.merge('RGB', (r,g,b))\n",
        "\n",
        "    mask = Image.merge('RGB', (r,g,b))\n",
        "\n",
        "\n",
        "if initt.mode == 'RGBA':\n",
        "    r1,g1,b1,a1 = initt.split()\n",
        "    rgb_image = Image.merge('RGB', (r1,g1,b1))\n",
        "\n",
        "\n",
        "    initt = Image.merge('RGB', (r1,g1,b1))\n",
        "    print(\"cl\")\n",
        "\n",
        "mask, initt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNDNccnS7Vfs"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clean Memory\n",
        "#@markdown There's a CUDA Memory error?</br> LUNCH IT !!!\n",
        "vae_tiny=None\n",
        "for _ in range(0,20):\n",
        "  fresh()"
      ],
      "metadata": {
        "id": "nlneFJ3SOM2U",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TuYxXYX2ESg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title generate\n",
        "\n",
        "\n",
        "if \"args\" in locals():\n",
        "    del args\n",
        "if \"imagess\" in locals():\n",
        "    del imagess\n",
        "if \"images\" in locals():\n",
        "    del images\n",
        "if \"out_preview\" in locals():\n",
        "    clear_output()\n",
        "    del out_preview\n",
        "if not(\"vae_tiny\" in locals()) or vae_tiny == None:\n",
        "    vae_load()\n",
        "if \"strength_freeU\" in locals() and strength_freeU>0:\n",
        "    pipe[\"model\"].unet.freeu.sd21()\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "args = {}\n",
        "\n",
        "preview = True #@param {type:\"boolean\"}\n",
        "prompt = \"Masterpiece photograph, a beautiful world in a bottle\" #@param {type:\"string\"}\n",
        "negPrompt = \"horrible, ugly, text, watermark\"}\n",
        "width = 600 #@param Integer\n",
        "height = 904 #@param Integer\n",
        "step = 44 #@param Integer\n",
        "seed = None #@param\n",
        "batch_size = 1 #@param Integer\n",
        "batch_number = 1 #@param Integer\n",
        "scheduler = \"EulerAncestralDiscreteScheduler\" #@param ['DDIMScheduler', 'DDPMScheduler', 'DEISMultistepScheduler', 'DPMSolverMultistepScheduler', 'DPMSolverSinglestepScheduler', 'EulerAncestralDiscreteScheduler', 'EulerDiscreteScheduler', 'HeunDiscreteScheduler', 'KDPM2AncestralDiscreteScheduler', 'KDPM2DiscreteScheduler', 'LMSDiscreteScheduler', 'PNDMScheduler', 'UniPCMultistepScheduler']\n",
        "strength = 3.7 #@param {type:\"slider\", min:0, max:30, step:0.1}\n",
        "#@markdown ---\n",
        "#@markdown #HighRes fix\n",
        "highres_fix = True #@param {type:\"boolean\"}\n",
        "scale = 1.75 #@param {type:\"slider\", min:0, max:2.5, step:0.05}\n",
        "denoise = 0.555 #@param {type:\"slider\", min:0, max:1, step:0.005}\n",
        "strength_hfx = 5.3 #@param {type:\"slider\", min:0, max:30, step:0.1}\n",
        "seed_hfx = None #@param\n",
        "step_hfx = 60 #@param Integer\n",
        "#@markdown ---\n",
        "#@markdown #Image2Image\n",
        "image2image = False #@param {type:\"boolean\"}\n",
        "inpainting = False #@param {type:\"boolean\"}\n",
        "imageStrength = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "imageBlur = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #ControlNet\n",
        "controlnet = False #@param {type:\"boolean\"}\n",
        "controlnet_strenght = 0.85 #@param {type:\"number\"}\n",
        "start_controlnet = 0.51 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "end_controlnet = 0.77 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "args['prompt'] = prompt\n",
        "args[\"negPrompt\"] = negPrompt\n",
        "args[\"width\"] = width\n",
        "args[\"height\"] = height\n",
        "args[\"step\"] = step\n",
        "args['seed'] = [seed-1] if str(seed)[1:].isnumeric() else [None]\n",
        "args[\"batch_size\"] = batch_size\n",
        "args[\"batch_number\"] = batch_number\n",
        "args[\"strength\"] = strength\n",
        "args[\"init_blur\"] = None\n",
        "args[\"controlnet_strenght\"] = controlnet_strenght if controlnet else 0.0\n",
        "args[\"controlnet_image\"] = image_controlnet if controlnet else None\n",
        "args[\"start_cnt\"] =  start_controlnet if controlnet else 0.0\n",
        "args[\"end_cnt\"] = end_controlnet if controlnet else 0.0\n",
        "args[\"ip_adap_en\"] = ip_adapter\n",
        "args[\"sdxl\"] = {}\n",
        "\n",
        "args['imageStrength'] = imageStrength\n",
        "\n",
        "if args[\"seed\"][0] == None:\n",
        "  args[\"seed\"] = [random.randint(-int(sys.maxsize),int(sys.maxsize))]\n",
        "args[\"cb\"] = cb if not(sdxl) else cbl\n",
        "\n",
        "pipe[\"model\"].scheduler = modifyScheduler(pipe,scheduler)\n",
        "\n",
        "hbox = widgets.HBox(children=[])\n",
        "out_preview = widgets.Output()\n",
        "display(out_preview)\n",
        "\n",
        "if image2image or inpainting:\n",
        "  args[\"init_image\"] = initt\n",
        "  args[\"init_image\"] = args[\"init_image\"].resize((width, height))\n",
        "  if (imageBlur):\n",
        "    args['init_blur'] = (initt.filter(ImageFilter.BLUR))\n",
        "    args['init_image'] = initt\n",
        "  args['init_blur'] = initt\n",
        "  args[\"mask_image\"] = mask.convert(\"RGBA\")\n",
        "\n",
        "  args['init_image'] = args['init_image'].resize((width, height))\n",
        "  args['init_blur'] =  args['init_blur'].resize((width, height))\n",
        "  args['mask_image'] = args['mask_image'].resize((width, height))\n",
        "else:\n",
        "  args['init_image'] = Image.new('RGBA', (width, height),  (255, 255, 255))\n",
        "  args['init_blur'] = Image.new('RGBA', (width, height),  (255, 255, 255))\n",
        "  args['mask_image'] = Image.new('RGBA', (width, height),  (255, 255, 255))\n",
        "  args['imageStrength'] = 1.0\n",
        "\n",
        "if sdxl:\n",
        "  args[\"sdxl\"] = {\"num_images_per_prompt\":batch_size}\n",
        "\n",
        "\n",
        "args[\"mxl\"] = int(min(max(len(args[\"prompt\"]), len(args[\"negPrompt\"]))/30, 77))\n",
        "args[\"scale_lora\"] = 1.0\n",
        "args[\"Pp\"] = \"image\" if image2image else (\"inpaint\" if inpainting else \"text\")\n",
        "\n",
        "fresh()\n",
        "fresh()\n",
        "fresh()\n",
        "fresh()\n",
        "\n",
        "images = generate(pipe[\"model\"],args)\n",
        "\n",
        "\n",
        "\n",
        "imagess=[]\n",
        "if highres_fix:\n",
        "  args[\"step\"] = step_hfx\n",
        "  args[\"ip_adap_en\"]=ip_adapter_highfix_pass\n",
        "  args[\"controlnet_strenght\"] = 0.0\n",
        "  args[\"controlnet_image\"] =  None\n",
        "  args[\"strength\"] = strength_hfx\n",
        "  for ima in images:\n",
        "    fresh()\n",
        "    o_w = int(ima.size[0]*scale)\n",
        "    o_h = int(ima.size[1]*scale)\n",
        "    args[\"init_image\"] = ima.resize((o_w,o_h))\n",
        "    args[\"width\"]=o_w\n",
        "    args[\"height\"]=o_h\n",
        "    args['imageStrength'] = denoise\n",
        "    args[\"batch_size\"] = 1\n",
        "    args[\"batch_number\"] = 1\n",
        "    if sdxl:\n",
        "      args[\"sdxl\"] = {\"num_images_per_prompt\":1}\n",
        "\n",
        "    if seed_hfx == None:\n",
        "      args[\"seed\"] = [random.randint(-int(sys.maxsize),int(sys.maxsize))]\n",
        "    else:\n",
        "      args['seed'] = [seed_hfx]\n",
        "    args[\"Pp\"] = \"image\"\n",
        "    fresh()\n",
        "    fresh()\n",
        "    imagess.append(generate(pipe[\"model\"],args)[0])\n",
        "  images=imagess\n",
        "\n",
        "\n",
        "with out_preview:\n",
        "    clear_output(wait=True)\n",
        "    for img in images:\n",
        "        display(img)\n",
        "del out_preview\n",
        "del hbox\n",
        "fresh()\n",
        "fresh()\n",
        "fresh()\n",
        "fresh()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSxH_fz9S3rU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Display images when connection loss\n",
        "for im in images:\n",
        "  display(im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stze8m9HclLK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Save image\n",
        "x= [X for X in os.listdir(\"/content/\") if \"IMG\" in X]\n",
        "x.sort()\n",
        "images[0].save(\"/content/IMG\"+str(len(x))+\".png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Copy images to drive\n",
        "if not(os.path.exists(\"/content/drive/MyDrive/imgs/\")):\n",
        "  os.mkdir(\"/content/drive/MyDrive/imgs/\")\n",
        "for e in  [X for X in os.listdir(\"/content/\") if \"IMG\" in X]:\n",
        "  print(e)\n",
        "  !cp \"{\"/content/\"+e}\" \"{\"/content/drive/MyDrive/imgs/lol\"+e}\""
      ],
      "metadata": {
        "id": "TlyYQ0bbjkBU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiP9aBdj9Euu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ADetailer\n",
        "\n",
        "#@markdown A first implementation of ADetailer, width and height will be the render size of pipe.</br> The pixel_increase_millimesimal increases the face frame as a millimesimal of the image size.</br> I raccomend to use a positive prompt with \"face, highly detailed face, character...\"\n",
        "\n",
        "import importlib\n",
        "importlib.reload(ADetailer)\n",
        "\n",
        "preview = False\n",
        "\n",
        "model = \"face_yolov8n_v2.pt\" #@param [\"face_yolov8n_v2.pt\",\"face_yolov8m.pt\",\"hand_yolov8n.pt\"]\n",
        "positive = \"Looking to viewer, highly detailed face, $PROMPT\" #@param {type:\"string\"}\n",
        "negative = \"Deformed face, red, $NPROMPT\" #@param {type:\"string\"}\n",
        "step = 40 #@param {type:\"integer\"}\n",
        "width = 960 #@param {type:\"integer\"}\n",
        "height = 960 #@param {type:\"integer\"}\n",
        "pixel_increase_millimesimal = 250 #@param {type:\"integer\", min :0, max:100}\n",
        "blur_radius = 10 #@param {type:\"integer\", min :0, max:100}\n",
        "denoise = 0.25 #@param {type:\"slider\", min:0, max:1.0, step:0.01}\n",
        "dilatation = 10 #@param {type:\"integer\", min:0, max:100.0, step:1}\n",
        "\n",
        "cfg =  8.1 #@param {type:\"slider\", min:0, max:30.0, step:0.05}\n",
        "\n",
        "#@markdown Reapeat recursively the diffusion every time the denoise decrease in function of decrease function\n",
        "recursively = 2 #@param {type:\"integer\", min:0, max:20}\n",
        "\n",
        "decrease_function = \"linear(x)->x-x/20\" #@param [\"constant(x)->x\",\"linear(x)->x-x/20\",\"linear(x)->x-x/10\",\"linear(x)->x-x/5\",\"linear(x)->x-x/2\"]\n",
        "\n",
        "def decrease(x):\n",
        "  x_n = x\n",
        "  if \"linear\" in decrease_function:\n",
        "    number = int(decrease_function.split(\"/\")[1])\n",
        "    x_n = x - x/number\n",
        "  return x_n\n",
        "\n",
        "args1 = {}\n",
        "\n",
        "args1[\"step\"] = step\n",
        "args1[\"denoise\"] = denoise\n",
        "args1[\"dilatation\"] = dilatation\n",
        "args1[\"cfg\"] = cfg\n",
        "args1[\"positive\"] = \"highly detailed face, \" + args[\"prompt\"] if positive==\"\" else (positive.replace(\"$PROMPT\",args[\"prompt\"]) if \"$PROMPT\" in positive else positive)\n",
        "args1[\"negative\"] = args[\"negPrompt\"] if negative==\"\" else (negative.replace(\"$NPROMPT\",args[\"negPrompt\"]) if \"$NPROMPT\" in negative else negative)\n",
        "args1[\"width\"] = width\n",
        "args1[\"height\"] = height\n",
        "args1[\"pipe\"] = pipe[\"model\"]\n",
        "args1[\"percent\"] = pixel_increase_millimesimal\n",
        "args1[\"blur_radius\"] = blur_radius\n",
        "args1[\"ip_adap_en\"] = ip_adapter\n",
        "\n",
        "if \"cb\" in args.keys():\n",
        "  args1[\"cb\"] = args[\"cb\"]\n",
        "if \"detailer\" in locals():\n",
        "  del detailer\n",
        "\n",
        "detailer = ADetailer.ADetailerDiffuser(model, 0.25)\n",
        "images1= [im.copy() for im in images]\n",
        "for i in range(0,recursively):\n",
        "  print(str(i)+\"/\"+str(recursively)+\" iteration (\"+str(args1[\"denoise\"])+\")\")\n",
        "  imagess = []\n",
        "  for img in images1:\n",
        "    if not(ip_adapter):\n",
        "      args1[\"pipe\"].enable_vae_slicing()\n",
        "    imagess.append(detailer(img, args1))\n",
        "\n",
        "  for im in (imagess):\n",
        "    display(im)\n",
        "    im.show()\n",
        "  fresh()\n",
        "  images1 = [ima.copy() for ima in imagess]\n",
        "  args1[\"denoise\"] = decrease(args1[\"denoise\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz7jdw_1iMSv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Save image\n",
        "x= [X for X in os.listdir(\"/content/\") if \"IMGD\" in X]\n",
        "x.sort()\n",
        "images1[0].save(\"/content/IMGD\"+str(len(x))+\".png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "N0cX26EZSZml",
        "PIAc0s5NIUo9",
        "WCJ5b_9LVBuw",
        "yxw_EHEFtB3g",
        "6dj4hW2gXLnB"
      ],
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
